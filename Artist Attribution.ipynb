{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9b482c2",
   "metadata": {},
   "source": [
    "# ECMM451 Data Science Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13875105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import string\n",
    "\n",
    "# Text preprocessing\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "import ast\n",
    "\n",
    "# Model helpers\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "from textblob import TextBlob\n",
    "from langdetect import detect_langs\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Models\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from collections import Counter\n",
    "from adjustText import adjust_text\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10430d93",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1289cff0",
   "metadata": {},
   "source": [
    "## 1.1. Spotify Calls\n",
    "- Step 1: Getting the token to access Spotify API\n",
    "- Step 2: Getting artist information. This dictionary was manually made by collecting data from Billoard\n",
    "- Step 3: Find all artists. Querying Spotify API to get artist information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff317b30",
   "metadata": {},
   "source": [
    "### Step 1: Getting the token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033641a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Spotify Client Id from https://developer.spotify.com/ and replce in spotify_client_id\n",
    "spotify_client_id = '<Your Spotify Client Id>'\n",
    "# Get Spotify Client Secret from https://developer.spotify.com/ and replce in spotify_client_secret\n",
    "spotify_client_secret = '<Your Spotify Client Secret>'\n",
    "\n",
    "spotify_account_base_url = \"https://accounts.spotify.com\"\n",
    "spotify_api_base_url = 'https://api.spotify.com/v1'\n",
    "spotify_token_url = spotify_account_base_url + \"/api/token\"\n",
    "spotify_headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n",
    "spotify_params = {'grant_type': 'client_credentials', \n",
    "                  'client_id': spotify_client_id, \n",
    "                  'client_secret': spotify_client_secret}\n",
    "\n",
    "spotify_response = requests.post(spotify_token_url, params=spotify_params, headers=spotify_headers)\n",
    "token_response = spotify_response.json()\n",
    "spotify_access_token = token_response['access_token']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c719b3",
   "metadata": {},
   "source": [
    "### Step 2: Get artist info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184296c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Year-wise list of artists collected from billboard #\n",
    "######################################################\n",
    "\n",
    "artists = {\n",
    "    '2022': [\n",
    "        \"Bad bunny\", \"Taylor Swift\", \"Harry Styles\", \"Drake\",\n",
    "        \"Morgan Wallen\", \"Doja Cat\", \"Ed Sheeran\", \"Adele\", \"The Weeknd\",\n",
    "        \"Lil Baby\", \"Future\", \"Justin Bieber\", \"Post Malone\", \"Jack Harlow\", \n",
    "        \"Kendrick Lamar\", \"Luke Combs\", \"Juice Wrld\", \"Glass Animals\", \"Lil Durk\", \n",
    "        \"Lil Nas X\", \"Dua Lipa\", \"Elton John\", \"YoungBoy never broke again\", \"Rod Wave\", \n",
    "        \"Kanye West\", \"Olivia Rodrigo\", \"Beyoncé\", \"The Kid LAROI\", \"Billie Eilish\", \n",
    "        \"Kodak Black\", \"Lizzo\", \"Gunna\", \"Imagine Dragons\", \"Latto\", \"Chris Stapleton\", \n",
    "        \"Summer Walker\", \"Walker Hayes\", \"Polo G\", \"SZA\", \"Bruno Mars\", \"Zach Bryan\",\n",
    "        \"Steve Lacy\", \"Eminem\", \"Bailey Zimmerman\", \"Tyler, the Creator\", \"Kate Bush\", \n",
    "        \"Karol G\", \"Kane Brown\", \"Gayle\", \"Nardo Wick\", \"Cole Swindell\", \"Ariana Grande\",\n",
    "        \"Lil Uzi Vert\", \"BTS\", \"Cody Johnson\",\"J. Cole\", \"Megan Thee Stallion\", \"Nicki Minaj\",\n",
    "        \"Chris Brown\", \"XXXTentacion\", \"Anderson .Paak\", \"Machine Gun Kelly\", \"Pop Smoke\",\n",
    "        \"The Beatles\", \"MoneyBagg Yo\", \"Fleetwood Mac\", \"Queen\", \"Jason Aldean\", \"Em Beihold\",\n",
    "        \"Michael Jackson\", \"OneRepublic\", \"Travis Scott\", \"PlayBoi Carti\", \"Giveon\",\n",
    "        \"Mariah Carey\", \"Lady Gaga\", \"Frank Ocean\", \"Yeat\", \"Red Hot Chili Peppers\", \n",
    "        \"Andy Williams\", \"Jordan Davis\", \"DJ Khaled\", \"JID\", \"Guns N' Roses\", \"Joji\",\n",
    "        \"Charlie Puth\", \"Michael Bublé\", \"Carrie Underwood\", \"Metallica\", \"Dove Cameron\",\n",
    "        \"Stephanie Beatriz\", \"Baby Keem\", \"Rauw Alejandro\", \"Brent Faiyaz\", \"Jessica Darrow\",\n",
    "        \"Chencho Corleone\", \"ColdPlay\", \"CKay\", \"Nirvana\", \"21 Savage\"\n",
    "    ],\n",
    "    '2021': [\n",
    "        \"DaBaby\", \"Young Thug\", \"Cardi B\", \"Gabby Barrett\", \"Pooh Shiesty\", \"24kGoldn\",\n",
    "        \"Lil Tjay\", \"Roddy Ricch\", \"Luke Bryan\", \"Masked Wolf\", \"Dan + Shay\", \"Thomas Rhett\",\n",
    "        \"AJR\", \"Kid Cudi\", \"H.E.R.\", \"Florida Georgia Line\", \"Khalid\", \"Maroon 5\", \"Lewis Capaldi\",\n",
    "        \"Ava Max\", \"Tate McRae\", \"Migos\", \"King Von\", \"Saweetie\", \"AC/DC\", \"Kali Uchis\", \n",
    "        \"Miley Cyrus\", \"blackbear\", \"Lee Brice\", \"Eagles\", \"Sam Smith\", \"Yung Bleu\", \"Miranda Lambert\", \n",
    "        \"Trippie Redd\", \"CJ\", \"Sam Hunt\", \"Jhené Aiko\", \"Eric Church\", \"Creedence Clearwater Revival\"\n",
    "    ],\n",
    "    '2020': [\n",
    "        \"Jonas Brothers\", \"Maren Morris\", \"Halsey\", \"Tones And I\", \"Selena Gomez\", \"Camila Cabello\",\n",
    "        \"Lil Mosey\", \"SAINt JHN\", \"Trevor Daniel\", \"Blake Shelton\", \"Arizona Zervas\", \"A Boogie Wit da Hoodie\",\n",
    "        \"Mustard\", \"Mac Miller\", \"Shawn Mendes\", \"JACKBOYS\", \"Céline Dion\", \"Don Toliver\", \"YNW Melly\",\n",
    "        \"Old Dominion\", \"Tory Lanez\", \"NF\", \"NLE Choppa\", \"Lil Wayne\", \"U2\", \"Surfaces\", \"Trans-Siberian Orchestra\",\n",
    "        \"Lil Tecca\", \"Jon Pardi\", \"SHAED\", \"Marshmello\", \"Kenny Chesney\", \"Maddie & Tae\"\n",
    "    ],\n",
    "    '2019': [\n",
    "        \"Panic! At The Disco\", \"Meek Mill\", \"P!nk\", \"Swae Lee\", \"The Rolling Stones\", \"5 Seconds Of Summer\",\n",
    "        \"Lauren Daigle\", \"Ella Mai\", \"Bradley Cooper\", \"Normani\", \"twenty one pilots\", \"Billy Ray Cyrus\",\n",
    "        \"Bastille\", \"Offset\", \"City Girls\", 'Blueface', \"Billy Joel\", \"Backstreet Boys\", \"Paul McCartney\",\n",
    "        \"Bazzi\", \"6ix9ine\", \"Sheck Wes\", \"Nipsey Hussle\", \"benny blanco\"\n",
    "    ],\n",
    "    '2018': [\n",
    "        \"Justin Timberlake\", \"Demi Lovato\", \"Bebe Rexha\", \"G-Eazy\", \"Lil Pump\", \"Logic\", \"JAY-Z\", \"J Balvin\",\n",
    "        \"Ozuna\", \"Brett Young\", \"Childish Gambino\", \"Portugal. The Man\", \"Lil Skies\", \"EXO\", \"BlocBoy JB\",\n",
    "        \"Lauv\", \"Keith Urban\", \"Metro Boomin\", \"Pentatonix\", \"Rihanna\", \"The Carters\", \"Ty Dolla $ign\", \n",
    "        \"Gucci Mane\", \"Journey\", \"Foo Fighters\"\n",
    "    ],\n",
    "    '2017': [\n",
    "        \"The Chainsmokers\", \"J. Cole\", \"Lady Gaga\", \"Alessia Cara\", \"Niall Horan\", \"Rae Sremmurd\",\n",
    "        \"Big Sean\", \"James Arthur\", \"Katy Perry\", \"Depeche Mode\", \"Luis Fonsi\", \"Maluma\", \"French Montana\", \n",
    "        \"Daddy Yankee\", \"Liam Payne\", \"Calvin Harris\", \"Zayn\", \"Julia Michaels\", \"Linkin Park\", \n",
    "        \"Chance The Rapper\", \"Bryson Tiller\", \"Quavo\", \"Tom Petty And The Heartbreakers\", \"Roger Waters\",\n",
    "        \"John Mayer\", \"2 Chainz\", \"Zac Brown Band\", \"Kesha\", \"Sia\", \"Zedd\"\n",
    "    ],\n",
    "    '2016': [\n",
    "        \"Bruce Springsteen\", \"Meghan Trainor\", \"Prince\", \"Desiigner\", \"Fetty Wap\",\n",
    "        \"One Direction\", \"Fifth Harmony\", \"Lukas Graham\", \"Flo Rida\", \"Kevin Gates\", \"DNCE\", \"Mike Posner\",\n",
    "        \"Daya\", \"Madonna\", \"Major Lazer\", \"Ellie Goulding\", \"James Bay\", \"Troye Sivan\", \"Jeremih\", \"Wiz Khalifa\",\n",
    "        \"X Ambassadors\", \"Britney Spears\", \"Elle King\", \"David Bowie\", \"Kiiara\", \"Tim McGraw\", \"Dierks Bentley\",\n",
    "        \"Disturbed\", \"Jennifer Lopez\", \"Nick Jonas\", \"gnash\", \"DRAM\"\n",
    "    ],\n",
    "    '2015': [\n",
    "        \"Mark Ronson\", \"WALK THE MOON\", \"Hozier\", \"Fall Out Boy\", \"Jason Derulo\", \"silentó\", \"OMI\", \"Tove Lo\",\n",
    "        \"Rachel Platten\", \"Andy Grammer\", \"Pitbull\", \"David Guetta\", \"Iggy Azalea\", \"Little Big Town\", \n",
    "        \"Trey Songz\", \"Shania Twain\", \"Omarion\", \"DJ Snake\", \"Ne-Yo\", \"Vance Joy\", \"Kid Ink\", \"Rich Homie Quan\",\n",
    "        \"Neil Diamond\", \"Mumford & Sons\", \"Lana Del Rey\", \"A$AP Rocky\", \"Usher\", \"Grateful Dead\", \"Garth Brooks\",\n",
    "        \"Kelly Clarkson\", \"Kanye West\", \"T-Wayne\", \"Enrique Iglesias\"\n",
    "    ],\n",
    "    '2014': [\n",
    "        \"Pharrell Williams\", \"Lorde\", \"John Legend\", \"Avicii\", \"MAGIC!\", \"Charli XCX\", \"Nico & Vinz\", \"Shakira\",\n",
    "        \"Passenger\", \"Brantley Gilbert\", \"Idina Menzel\", \"Lady A\", \"YG\", \"American Authors\", \"Juicy J\", \"ScHoolboy Q\",\n",
    "        \"George Strait\", \"Paramore\", \"Austin Mahone\", \"Snoop Dogg\", \"Aloe Blacc\", \"Martin Garrix\", \"Disclosure\",\n",
    "        \"Romeo Santos\", \"A Great Big World\", \"Lil Jon\", \"Arctic Monkeys\", \"Cher\", \"Becky G\", \n",
    "        \"Bob Marley & The Wailers\", \"Rascal Flatts\", \"MKTO\", \"Sara Bareilles\", \"Christina Aguilera\"\n",
    "    ],\n",
    "    '2013': [\n",
    "        \"Macklemore & Ryan Lewis\", \"Robin Thicke\", \"The Lumineers\", \"Baauer\", \"Phillip Phillips\", \"Bon Jovi\",\n",
    "        \"Daft Punk\", \"Hunter Hayes\", \"Alicia Keys\", \"fun.\", \"Kesha\", \"will.i.am\", \"The Band Perry\", \"Miguel\",\n",
    "        \"Darius Rucker\", \"Anna Kendrick\", \"AWOLNATION\", \"T.I.\", \"Capital Cities\", \"Of Monsters And Men\",\n",
    "        \"Rod Stewart\", \"Swedish House Mafia\", \"Wanz\", \"Avril Lavigne\", \"Brad Paisley\", \"Wale\", \"Muse\",\n",
    "        \"Randy Houser\", \"Icona Pop\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc6d964",
   "metadata": {},
   "source": [
    "### Step 3: Find all artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac8ad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_headers = {'Authorization': f'Bearer {spotify_access_token}'}\n",
    "# To make sure no artist gets repeated\n",
    "unique_artists = []\n",
    "artist_search_params = {\n",
    "    'q': \"\",\n",
    "    'type': \"artist\",\n",
    "    'market': 'GB',\n",
    "    'limit': 10,\n",
    "}\n",
    "artists_info_df = pd.DataFrame(columns=['Artist_Name', 'Arist_Id', 'Genres', 'Followers', 'Popularity'])\n",
    "\n",
    "for key, value in artists.items():\n",
    "    print(f'Year: {key}')\n",
    "    print(len(value))\n",
    "    for artist in value:\n",
    "        if artist not in unique_artists:\n",
    "            print(artist)\n",
    "            artist_search_params['q'] = f'artist:{artist}'\n",
    "            # https://api.spotify.com/v1/search\n",
    "            spotify_artist_url = spotify_api_base_url + '/search'\n",
    "            spotify_artist_response = requests.get(spotify_artist_url, \n",
    "                                                   params = artist_search_params, \n",
    "                                                   headers=auth_headers)\n",
    "\n",
    "            json_response = spotify_artist_response.json()\n",
    "            artist_found = False\n",
    "            \n",
    "            for item in json_response['artists']['items']:\n",
    "                if item['name'].lower() == artist.lower():\n",
    "                    json_item = item\n",
    "                    artist_found = True\n",
    "                    break\n",
    "            # json_item = json_response['artists']['items'][0]\n",
    "\n",
    "            if artist_found == True:\n",
    "                artist_info = {\n",
    "                    'Artist_Name': json_item['name'],\n",
    "                    'Arist_Id': json_item['id'],\n",
    "                    'Genres': str(json_item['genres']),\n",
    "                    'Followers': json_item['followers']['total'],\n",
    "                    'Popularity': json_item['popularity']\n",
    "                }\n",
    "\n",
    "                artist_info_df = pd.DataFrame([artist_info])\n",
    "                artists_info_df = pd.concat([artists_info_df, artist_info_df], ignore_index=True)\n",
    "\n",
    "            else:\n",
    "                print(\"not found!!!!!!!!\")\n",
    "                print(f\"{json_response['artists']['items'][0]['name'].lower()} != {artist.lower()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861dbbe1",
   "metadata": {},
   "source": [
    "## 1.2. Genius.com Calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2102fb0",
   "metadata": {},
   "source": [
    "### Step 1: Getting genius id for artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b3b4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get API Token from https://docs.genius.com/ and replce in TOKEN\n",
    "TOKEN = '<Your Genius Token>'\n",
    "base_url = \"http://api.genius.com\"\n",
    "#using the API\n",
    "genius_headers = {'Authorization': f'Bearer {TOKEN}'}\n",
    "search_url = base_url + \"/search\"\n",
    "artist_url = base_url + \"/artists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57708a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get genius id of artist\n",
    "genius_artist_id = {}\n",
    "\n",
    "for key, value in artists.items():\n",
    "    print(f'Year: {key}')\n",
    "    for artist in value:\n",
    "        artist_params = {\n",
    "            'q': artist\n",
    "        }\n",
    "        response = requests.get(search_url, params = artist_params, headers=genius_headers)\n",
    "        artist_search_response = response.json()\n",
    "        for item in artist_search_response['response']['hits']:\n",
    "            flag = False\n",
    "            if item['result']['primary_artist']['name'].lower() == artist.lower():\n",
    "                flag = True\n",
    "                genius_artist_id[artist] = item['result']['primary_artist']['id']\n",
    "\n",
    "            if flag == True:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119b9657",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_df = pd.DataFrame(columns=['Artist_Name', '', 'Title', 'Lyrics_Url', 'Language'])\n",
    "\n",
    "artist_params = {'sort': 'popularity'}\n",
    "\n",
    "# Fetching Lyrics\n",
    "# Step 1: Get all links to lyrics \n",
    "# Fetching title for each artist based on popularity. API by default returns 20 results per page.\n",
    "\n",
    "for key, value in genius_artist_id.items():\n",
    "    page = 1\n",
    "    \n",
    "    while True:\n",
    "        print(key)\n",
    "        print(page)\n",
    "        artist_params['page'] = page\n",
    "        response = requests.get(f'{artist_url}/{value}/songs', params = artist_params, headers=genius_headers)\n",
    "        artist_songs_response = response.json()\n",
    "\n",
    "        for song in artist_songs_response['response']['songs']:\n",
    "            print(song['title'])\n",
    "            lyrics_info_dict = {'Artist_Name': key, \n",
    "                                'Featured_Artists': [],\n",
    "                                'Title': song['title'], \n",
    "                                'Lyrics_Url': song['url'], \n",
    "                                'Language': song['language']}\n",
    "            \n",
    "            if song['featured_artists'] != []:\n",
    "                for featured_artists in song['featured_artists']:\n",
    "                    lyrics_info_dict['Featured_Artists'].append(featured_artists['name'])\n",
    "\n",
    "            lyrics_info_df = pd.DataFrame([lyrics_info_dict])\n",
    "            lyrics_df = pd.concat([lyrics_df, lyrics_info_df], ignore_index=True)\n",
    "        \n",
    "        page += 1\n",
    "        \n",
    "        if not artist_songs_response['response']['next_page']:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ff4b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2:Use links stored in Dataframe to get lyrics\n",
    "\n",
    "all_artists = lyrics_df['Artist_Name'].unique()\n",
    "processed_artist = dict.fromkeys(all_artists, False)\n",
    "\n",
    "lyrics_df_list_by_artist = []\n",
    "\n",
    "for artist in all_artists:\n",
    "    lyrics_df_list_by_artist.append(lyrics_df[(lyrics_df[\"Artist_Name\"]==artist)])\n",
    "\n",
    "error_df = pd.DataFrame()\n",
    "error_idx = []\n",
    "\n",
    "option = Options()\n",
    "option.headless = True\n",
    "option.incognito = True\n",
    "browser = webdriver.Chrome(options=option)\n",
    "\n",
    "for idx, lyrics_df_by_artist in enumerate(lyrics_df_list_by_artist):\n",
    "    lyrics = []\n",
    "    no_error = True\n",
    "   \n",
    "    if not os.path.exists(f\"./artists/{lyrics_df_by_artist['Artist_Name'].iloc[0]}.pkl\"):\n",
    "        print(lyrics_df_by_artist['Artist_Name'].iloc[0])\n",
    "        if processed_artist[lyrics_df_list_by_artist[idx]['Artist_Name'].iloc[0]] == False:\n",
    "            for index, song in lyrics_df_by_artist.iterrows():\n",
    "                try:\n",
    "                    print('working on index: ', index)\n",
    "                    browser.get(song['Lyrics_Url'])\n",
    "                    titles_element = browser.find_element(\"xpath\", '/html/body/div[1]/main/div[2]/div[3]/div/div/div[3]')\n",
    "                    lyrics.append(titles_element.text)\n",
    "                    browser.back()\n",
    "                except Exception as e:\n",
    "                    browser.quit()\n",
    "                    os.system(\"taskkill /im chromedriver.exe\")\n",
    "                    browser = webdriver.Chrome(options=option)\n",
    "                    print(\"error!! index: \", index)\n",
    "                    error_idx.append(index)\n",
    "                    error_df = pd.concat([error_df,song])\n",
    "                    lyrics.append('')\n",
    "                    no_error = False\n",
    "                    continue\n",
    "\n",
    "            if no_error == True:\n",
    "                processed_artist[lyrics_df_list_by_artist[idx]['Artist_Name'].iloc[0]] = True\n",
    "\n",
    "            lyrics_df_by_artist['Lyrics'] = lyrics\n",
    "            if lyrics_df_by_artist['Artist_Name'].iloc[0] == \"AC/DC\":\n",
    "                lyrics_df_by_artist.to_pickle(f\"./artists/AC_DC.pkl\")\n",
    "            else:\n",
    "                lyrics_df_by_artist.to_pickle(f\"./artists/{lyrics_df_by_artist['Artist_Name'].iloc[0]}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259db348",
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_data_df = pd.DataFrame()\n",
    "\n",
    "directory = \"path/to/stored/pickle/files\"\n",
    "    \n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\".pkl\"): \n",
    "        print(filename)\n",
    "        artist_df = pd.read_pickle(os.path.join(directory, filename))\n",
    "        artist_data_df = artist_data_df.append(artist_df, ignore_index=True, verify_integrity=False, sort=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0be727d",
   "metadata": {},
   "source": [
    "## Note: Web Scraping takes more than 4 days without multi-processing. Collected and pre-processed dataset has been saved and can be provided if asked for. Size of zip file of data is more than limit on eBART"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3383b70b",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899e48b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_songs_artists(df):\n",
    "    # Group the DataFrame by 'artist' and count the number of songs for each artist\n",
    "    artist_song_counts = df.groupby('Artist_Name')['Title'].count()\n",
    "\n",
    "    # Sort artists by song counts\n",
    "    sorted_artists = artist_song_counts.sort_values()\n",
    "\n",
    "    # Select top 5, middle 5, and bottom 5 artists\n",
    "    top_artists = sorted_artists.tail(5)\n",
    "    middle_artists = sorted_artists[len(sorted_artists) // 2 - 2:len(sorted_artists) // 2 + 3]\n",
    "    bottom_artists = sorted_artists.head(5)\n",
    "\n",
    "    # Create a bar plot for top 5, middle 5, and bottom 5 artists\n",
    "    plt.figure(figsize=(10, 6))  # Set the figure size\n",
    "    plt.bar(top_artists.index, top_artists.values, color='darkgreen', label='Top 5', alpha=0.7)\n",
    "    plt.bar(middle_artists.index, middle_artists.values, color='mediumseagreen', label='Middle 5', alpha=0.7)\n",
    "    plt.bar(bottom_artists.index, bottom_artists.values, color='lightgreen', label='Bottom 5', alpha=0.7)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Artist')\n",
    "    plt.ylabel('Number of Songs')\n",
    "    plt.title('Number of Songs Vs Artists')\n",
    "    \n",
    "    plt.yscale('log')\n",
    "\n",
    "    # Customize the tick labels\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks()\n",
    "\n",
    "    # Add legend\n",
    "    plt.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83786781",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "################################\n",
    "#.        Network Graph        #\n",
    "################################\n",
    "################################\n",
    "def remove_other_artists(artist_df):\n",
    "    unique_artist_values = artist_df['Artist_Name'].unique().tolist()\n",
    "    artist_df_copy = artist_df.copy()\n",
    "    for index, row in artist_df_copy.iterrows():\n",
    "        upd_list = []\n",
    "        for artist in row['Featured_Artists']:\n",
    "            if artist in unique_artist_values:\n",
    "                if artist != row['Artist_Name']:\n",
    "                    upd_list.append(artist)\n",
    "        \n",
    "        artist_df_copy.at[index, 'Featured_Artists'] = upd_list\n",
    "        \n",
    "    return artist_df_copy\n",
    "\n",
    "\n",
    "df_saved = pd.read_pickle(\"./Datasets/finalproject/network_df.pkl\")\n",
    "\n",
    "\n",
    "df = remove_other_artists(df_saved)\n",
    "df = df[ df['Featured_Artists'].apply(lambda x: len(x) != 0)]\n",
    "\n",
    "\n",
    "df_graph= df[['Artist_Name', 'Featured_Artists']].copy()\n",
    "df_graph = df_graph.explode('Featured_Artists')\n",
    "\n",
    "\n",
    "#################################################\n",
    "\n",
    "# Group by 'Artist_Name' and 'Featured_Artists' and count the retweet frequency\n",
    "song_counts = df_graph.groupby(['Artist_Name', 'Featured_Artists']).size().reset_index(name='song_count')\n",
    "\n",
    "# Calculate the weights based on retweet frequency\n",
    "song_counts['weight'] = song_counts['song_count']\n",
    "\n",
    "################################################\n",
    "\n",
    "# Reset the index\n",
    "song_counts = song_counts.reset_index(drop=True)\n",
    "\n",
    "# Turn df into graph\n",
    "G = nx.from_pandas_edgelist(retweet_counts, 'Artist_Name', 'Featured_Artists', edge_attr='weight',  create_using=nx.DiGraph()) \n",
    "pos = nx.spring_layout(G,k=0.1) #specify layout for visual\n",
    "\n",
    "\n",
    "f, ax = plt.subplots(figsize=(10, 10))\n",
    "plt.style.use('ggplot')\n",
    "nodes = nx.draw_networkx_nodes(G, pos,node_size=20,alpha=0.8)\n",
    "nodes.set_edgecolor('k')\n",
    "nx.draw_networkx_edges(G, pos, width=0.5, alpha=0.2, edge_color='black')\n",
    "\n",
    "\n",
    "# Get a list of nodes\n",
    "nodes = list(G.nodes())\n",
    "# Get a list of edges with their corresponding weights\n",
    "edges = [(u, v, G[u][v]['weight']) for u, v in G.edges()]\n",
    "# Create a DataFrame for nodes\n",
    "nodes_df = pd.DataFrame(nodes, columns=['ID'])\n",
    "# Create a DataFrame for edges\n",
    "edges_df = pd.DataFrame(edges, columns=['Source', 'Target', 'Weight'])\n",
    "# Save nodes and edges to CSV\n",
    "nodes_df.to_csv('./nodes.csv', index=False)\n",
    "edges_df.to_csv('./edges.csv', index=False)\n",
    "\n",
    "# CSV files created were used in Gephi to plot the network graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dec6d9",
   "metadata": {},
   "source": [
    "# Pre processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849a6e1b",
   "metadata": {},
   "source": [
    "### Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b849d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix issue with trap and rap\n",
    "def genre_pre_processing(genre_list):\n",
    "    print(genre_list)\n",
    "    genre_list = ast.literal_eval(genre_list)\n",
    "    genre_list = [n.strip() for n in genre_list]\n",
    "    \n",
    "    genre_tracker = {key: False for key in genre_list}\n",
    "\n",
    "    main_genres = ['pop', 'rock', 'trap', 'hip hop', 'rap', 'dance', 'r&b', 'indie', 'country', 'metal', 'permanent wave', 'reggaeton', 'house', 'neo mellow', 'afrofuturism', 'soul', 'reggae', 'movie tunes']\n",
    "    genre_array = np.zeros(len(main_genres), dtype = int)\n",
    "    \n",
    "    # secondary genres\n",
    "    secondary_genres = {\n",
    "        'edm': 'dance',\n",
    "        'techno': 'dance',\n",
    "        'alt z': 'hip hop',\n",
    "        'mellow gold': 'rock',\n",
    "        'drill': 'hip hop',\n",
    "        'adult standards': 'pop',\n",
    "        'hollywood': 'pop'\n",
    "    }\n",
    "    \n",
    "    new_genre_list = []\n",
    "    \n",
    "    for genre in genre_list:\n",
    "        indices = []\n",
    "        genres_found = []\n",
    "        for main_genre in main_genres:\n",
    "            if main_genre in genre:\n",
    "                indices.append(genre.find(main_genre))\n",
    "                genres_found.append(main_genre)\n",
    "                \n",
    "        for secondary_genre in list(secondary_genres.keys()):\n",
    "            if secondary_genre in genre:\n",
    "                indices.append(genre.find(secondary_genre))\n",
    "                genres_found.append(secondary_genres[secondary_genre])\n",
    "                \n",
    "        \n",
    "        if indices != []:\n",
    "            new_genre_list.append(genres_found[np.argmin(indices)])\n",
    "            genre_tracker[genre] = True\n",
    "    \n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    upd_genres = [x for x in new_genre_list if not (x in seen or seen_add(x))]\n",
    "    \n",
    "    for genre in upd_genres:\n",
    "        genre_array[main_genres.index(genre)] = 1\n",
    "    \n",
    "    return genre_array\n",
    "\n",
    "def invoke_genre_pre_processing(song_df):\n",
    "    song_df_copy = song_df.copy()\n",
    "    song_df_copy['Genres_array'] = song_df['Genres'].apply(genre_pre_processing)    \n",
    "    return song_df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c3a3b9",
   "metadata": {},
   "source": [
    "### Profanity and Politeness Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47355530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profanity_score(lyrics):\n",
    "    words = lyrics.split()\n",
    "    curse_words = open(\"./curse\").read().splitlines()\n",
    "    \n",
    "    word_sum = 0\n",
    "    for word in words:\n",
    "        if word in curse_words:\n",
    "            word_sum += 1\n",
    "\n",
    "    return (word_sum/ len(lyrics))*100\n",
    "\n",
    "def politeness_score(lyrics):\n",
    "    words = lyrics.split()\n",
    "    love_words = open(\"./love_affection\").read().splitlines()\n",
    "    \n",
    "    word_sum = 0\n",
    "    for word in words:\n",
    "        if word in love_words:\n",
    "            word_sum += 1\n",
    "\n",
    "    return (word_sum/ len(lyrics))*100\n",
    "\n",
    "def invoke_profanity_politeness_score(song_df, col_name):\n",
    "    song_df_copy = song_df.copy()\n",
    "    \n",
    "    song_df_copy['profanity'] = song_df[col_name].apply(lambda x: profanity_score(x))\n",
    "    song_df_copy['politeness'] = song_df[col_name].apply(lambda x: politeness_score(x))\n",
    "    \n",
    "    return song_df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58e0fd2",
   "metadata": {},
   "source": [
    "### Lyrics Preprocessing Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee518c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_eng_and_songs_without_lyrics(song_df, col_name):\n",
    "    string_lyrics_not_avail = \"Lyrics for this song have yet to be released. Please check back once the song has been released\"\n",
    "    string_instrumental = \"This song is an instrumental\"\n",
    "    song_df_copy = song_df.copy()\n",
    "    song_df_copy = song_df_copy[ song_df_copy['Language'] == 'en']\n",
    "    song_df_copy = song_df_copy[ [string_lyrics_not_avail not in g for g in song_df_copy[col_name]]]\n",
    "    song_df_copy = song_df_copy[ [string_instrumental not in g for g in song_df_copy[col_name]]]\n",
    "    song_df_copy = song_df_copy[ song_df_copy[col_name].apply(lambda x: len(x.split(' ')) > 50) ]\n",
    "        \n",
    "    return song_df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912f08b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_round_brackets(song_df, col_name):\n",
    "    song_df_copy = song_df.copy()\n",
    "    song_df_copy['Lyrics_without_rb'] = song_df[col_name].map(lambda s: re.sub(r'(\\(|\\))', '', s))\n",
    "    song_df_copy.drop([col_name], axis=1, inplace=True)\n",
    "    \n",
    "    return song_df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7dbf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_square_brackets(song_df, col_name):\n",
    "    song_df_copy = song_df.copy()\n",
    "    song_df_copy['Lyrics_without_sb'] = song_df[col_name].map(lambda s: re.sub(r\"\\[[^\\[\\]]*\\]\", '', s))\n",
    "    song_df_copy.drop([col_name], axis=1, inplace=True)\n",
    "    \n",
    "    return song_df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc77fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lyrics_to_sentences(lyrics):\n",
    "    sentences = lyrics.split('\\n')\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def fetch_lines_from_lyrics(song_df, col_name):\n",
    "    song_df_copy = song_df.copy()\n",
    "    song_df_copy['Lyrics_lines'] = song_df[col_name].apply(convert_lyrics_to_sentences).tolist()\n",
    "    song_df_copy.drop([col_name], axis=1, inplace=True)\n",
    "    \n",
    "    return song_df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff158314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_new_line_character(lyrics):\n",
    "    # Replace new line characters in the middle with spaces\n",
    "    lyrics = lyrics.replace('\\n', ' ')\n",
    "    # Remove new line characters at the start or end of the text\n",
    "    lyrics = lyrics.strip('\\n')\n",
    "    \n",
    "    return lyrics\n",
    "\n",
    "def fetch_nlc_removed_lyrics(song_df, col_name):\n",
    "    song_df_copy = song_df.copy()\n",
    "    song_df_copy['Lyrics_without_nlc'] = song_df[col_name].apply(remove_new_line_character)\n",
    "    song_df_copy.drop([col_name], axis=1, inplace=True)\n",
    "    \n",
    "    return song_df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36910752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(lyrics):\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    lyrics_without_punct = lyrics.translate(translator)\n",
    "    \n",
    "    return lyrics_without_punct\n",
    "\n",
    "def token_stemming(lyrics_tokens):\n",
    "    tokenized_words = word_tokenize(lyrics_tokens)\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = [stemmer.stem(word) for word in tokenized_words]\n",
    "    \n",
    "    return stemmed_words\n",
    "    \n",
    "\n",
    "def to_lower_and_remove_punctuations(song_df, col_name):\n",
    "    song_df_copy = song_df.copy()\n",
    "    \n",
    "    # Clean data using nltk\n",
    "    song_df_copy['Lyrics_clean'] = song_df[col_name].apply(lambda x: remove_punctuation(x.lower()))\n",
    "    song_df_copy['Lyrics_clean'] = song_df_copy['Lyrics_clean'].apply(lambda x: token_stemming(x))\n",
    "    \n",
    "    song_df_copy.drop([col_name], axis=1, inplace=True)\n",
    "    return song_df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04afc620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_line_with_substring(lyrics):    \n",
    "    patterns = [r'Translations', r'\\d+', r'\\w+ Contributors', r'\\w+ Lyrics', r'Embed']\n",
    "    lines = lyrics.splitlines()\n",
    "    filtered_lines = []\n",
    "    for line in lines:\n",
    "        match = False\n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, line)\n",
    "            if match:\n",
    "                match = True\n",
    "                break\n",
    "        \n",
    "        if not match:\n",
    "            filtered_lines.append(line)\n",
    "    updated_lyrics = '\\n'.join(filtered_lines)\n",
    "    return updated_lyrics\n",
    "\n",
    "def invoke_remove_line_with_substring(song_df, col_name):\n",
    "    song_df_copy = song_df.copy()\n",
    "    \n",
    "    song_df_copy['Lyrics_without_junk'] = song_df[col_name].apply(remove_line_with_substring)\n",
    "    song_df_copy.drop([col_name], axis=1, inplace=True)\n",
    "    \n",
    "    return song_df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79efc58",
   "metadata": {},
   "source": [
    "### Lyrics Preprocessing End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a384dd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to set seed value for word2vec\n",
    "def hash(astring):\n",
    "    return ord(astring[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c38a00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Word2Vec_vectorize_complete_song(X_train, X_test, y_train, y_test, vector_size = 100, window = 25): \n",
    "    # Generating Word2Vec Model\n",
    "    w2v_model = gensim.models.Word2Vec(X_train,\n",
    "                                       min_count=2,\n",
    "                                       window=window,\n",
    "                                       vector_size=vector_size,\n",
    "                                       sample=6e-5, \n",
    "                                       alpha=0.03, \n",
    "                                       min_alpha=0.0007, \n",
    "                                       workers=4, \n",
    "                                       hashfxn=hash)\n",
    "            \n",
    "    return w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ac1d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dimensions(model):\n",
    "    num_dimensions = 2  # final num dimensions (2D, 3D, etc)\n",
    "\n",
    "    # extract the words & their vectors, as numpy arrays\n",
    "    vectors = np.asarray(model.wv.vectors)\n",
    "    labels = np.asarray(model.wv.index_to_key)  # fixed-width numpy strings\n",
    "    \n",
    "    # Reduces the dimensionality from 300 to 50 dimensions with PCA\n",
    "    reduc = PCA(n_components=50).fit_transform(vectors)\n",
    "\n",
    "    # reduce using t-SNE\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0, perplexity=15) #.fit_transform(reduc))\n",
    "    vectors = tsne.fit_transform(reduc)\n",
    "\n",
    "    x_vals = [v[0] for v in vectors]\n",
    "    y_vals = [v[1] for v in vectors]\n",
    "    return x_vals, y_vals, labels\n",
    "\n",
    "def plot_with_matplotlib(sentence_model, res_df):    \n",
    "    curse_words = open(\"./curse\").read().splitlines()\n",
    "    love_words = open(\"./love_affection\").read().splitlines()\n",
    "    \n",
    "    # Making vocab_with_freq\n",
    "    # Flatten the lists in the 'words_column' and count the frequency of each word\n",
    "    word_list = [word for sublist in res_df['Lyrics_clean'] for word in sublist]\n",
    "    word_frequency = Counter(word_list)\n",
    "    \n",
    "    ###############\n",
    "    # Curse words #\n",
    "    ###############\n",
    "    selected_words = curse_words\n",
    "    \n",
    "    # Filter the word_frequency dictionary to keep only selected words\n",
    "    filtered_word_frequency = {word: count for word, count in word_frequency.items() if word in selected_words}\n",
    "    # new addition\n",
    "    # Get the top 10 most common elements based on frequency\n",
    "    # Sort the dictionary items by values in descending order\n",
    "    sorted_items = sorted(filtered_word_frequency.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "    # Keep only the top 10 elements\n",
    "    filtered_word_frequency = dict(sorted_items[:20])\n",
    "    total_frequency = sum(filtered_word_frequency.values())\n",
    "\n",
    "    # Normalize the frequencies\n",
    "    vocab_with_freq_curse = {word: (count / total_frequency)*1500 for word, count in filtered_word_frequency.items()}\n",
    "\n",
    "    \n",
    "    ###############\n",
    "    # Love words #\n",
    "    ###############\n",
    "    selected_words = love_words\n",
    "    \n",
    "    # Filter the word_frequency dictionary to keep only selected words\n",
    "    filtered_word_frequency = {word: count for word, count in word_frequency.items() if word in selected_words}\n",
    "    # new addition\n",
    "    # Get the top 10 most common elements based on frequency\n",
    "    # Sort the dictionary items by values in descending order\n",
    "    sorted_items = sorted(filtered_word_frequency.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "    # Keep only the top 10 elements\n",
    "    filtered_word_frequency = dict(sorted_items[:20])\n",
    "    total_frequency = sum(filtered_word_frequency.values())\n",
    " \n",
    "    # Normalize the frequencies\n",
    "    vocab_with_freq_love = {word: (count / total_frequency)*1500 for word, count in filtered_word_frequency.items()}\n",
    "\n",
    "    \n",
    "    print(\"INFO: vocab_with_freq populated\")\n",
    "\n",
    "    x_vals, y_vals, labels = reduce_dimensions(sentence_model)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    \n",
    "    indices = list(range(len(labels)))\n",
    "    selected_indices_curse = []\n",
    "    selected_indices_love = []\n",
    "    \n",
    "    annotations_objs = []\n",
    "    \n",
    "    for curse_word in vocab_with_freq_curse.keys():\n",
    "        if curse_word in labels:\n",
    "            selected_indices_curse.append(np.where(labels == curse_word)[0][0])\n",
    "            \n",
    "\n",
    "    for love_word in vocab_with_freq_love.keys():\n",
    "        if love_word in labels:\n",
    "            selected_indices_love.append(np.where(labels == love_word)[0][0])\n",
    "            \n",
    "    selected_indices_curse = np.array(selected_indices_curse)\n",
    "    selected_indices_love = np.array(selected_indices_love)\n",
    "    \n",
    "    for i in selected_indices_curse:\n",
    "        cu = plt.scatter(x_vals[i], y_vals[i], s = vocab_with_freq_curse[labels[i]], c = '#b9cdeb')\n",
    "        annotations_objs.append(plt.annotate(labels[i], (x_vals[i], y_vals[i]), color='black', fontsize=10))\n",
    "        \n",
    "    for i in selected_indices_love:\n",
    "        lo = plt.scatter(x_vals[i], y_vals[i], s = vocab_with_freq_love[labels[i]], c = '#f7c500')\n",
    "        annotations_objs.append(plt.annotate(labels[i], (x_vals[i], y_vals[i]), color='black', fontsize=10))\n",
    "        \n",
    "    adjust_text(annotations_objs)\n",
    "    \n",
    "    plt.legend((cu, lo),\n",
    "               ('Profanity', 'Love/Affection'),\n",
    "               loc='upper right',\n",
    "               ncol=2,\n",
    "               fontsize=12,\n",
    "               markerscale = 5,\n",
    "               framealpha = 0.5)\n",
    "    \n",
    "    plt.axis('tight')\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Profanity And Love Clusting: Word2Vec\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ab50ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to plot accuracy of RF using word2vec against value chose for vector size and window size\n",
    "# This was used for hyperparameter tuning of word2vec\n",
    "def plot_acc_vs_window(vector_size, window_size, acc):\n",
    "    plt.figure()\n",
    "    \n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.xlabel(\"window size\")\n",
    "    \n",
    "    for vec_s in vector_size:\n",
    "        plt.plot(window_size, acc[vec_s], marker='o', label='%s :vector size' % vec_s)\n",
    "        \n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a48d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Classify_RF(X_train_vect_avg, train_genres_array, train_profanity, train_politeness, X_test_vect_avg, test_genres_array, test_profanity, test_politeness, y_train, y_test):\n",
    "########################################\n",
    "#     #Hyperparameter Tuning           #\n",
    "########################################\n",
    "#     # Number of trees in random forest\n",
    "#     n_estimators = [int(x) for x in np.linspace(start = 50, stop = 200, num = 4)]\n",
    "#     # Number of features to consider at every split\n",
    "#     max_features = ['auto', 'sqrt']\n",
    "#     # Maximum number of levels in tree\n",
    "#     max_depth = [int(x) for x in np.linspace(5, 20, num = 4)]\n",
    "#     max_depth.append(None)\n",
    "#     # Minimum number of samples required to split a node\n",
    "#     min_samples_split = [2, 4, 6]\n",
    "#     # Minimum number of samples required at each leaf node\n",
    "#     min_samples_leaf = [1, 2, 4]\n",
    "#     # Method of selecting samples for training each tree\n",
    "#     bootstrap = [True, False]\n",
    "#     # Create the random grid\n",
    "#     random_grid = {'n_estimators': n_estimators,\n",
    "#                    'max_features': max_features,\n",
    "#                    'max_depth': max_depth,\n",
    "#                    'min_samples_split': min_samples_split,\n",
    "#                    'min_samples_leaf': min_samples_leaf,\n",
    "#                    'bootstrap': bootstrap}\n",
    "#     print(random_grid)\n",
    "    \n",
    "#     # Use the random grid to search for best hyperparameters\n",
    "#     # First create the base model to tune\n",
    "#     rf_random = RandomizedSearchCV(estimator = rf, \n",
    "#                                    param_distributions = random_grid, \n",
    "#                                    n_iter = 100, \n",
    "#                                    verbose=2, \n",
    "#                                    random_state=42, \n",
    "#                                    n_jobs = -1)\n",
    "#     Fit the random search model\n",
    "#     rf_random.fit(X_train_vect_avg, y_train.values.ravel())\n",
    "#     rf_model = rf_random.best_estimator_\n",
    "#     # Best Parameters found using grid search : {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': None, 'bootstrap': False}\n",
    "########################################\n",
    "#     #Hyperparameter Tuning           #\n",
    "########################################\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators = 200, \n",
    "                                min_samples_split = 2, \n",
    "                                min_samples_leaf = 1, \n",
    "                                max_features = 'auto', \n",
    "                                max_depth = None, \n",
    "                                bootstrap = False, \n",
    "                                n_jobs = -1, \n",
    "                                random_state = random.seed(1))\n",
    "    print(\"INFO: rf_model created\")\n",
    "    \n",
    "    # Uncomment to use the desired set of features\n",
    "    # X_train = X_train_vect_avg\n",
    "    # X_train = np.column_stack((X_train_vect_avg, train_profanity, train_politeness))\n",
    "    X_train = np.column_stack((X_train_vect_avg, train_profanity, train_politeness, train_genres_array))\n",
    "        \n",
    "    rf_model = rf.fit(X_train, y_train.values.ravel())\n",
    "    print(\"INFO: rf_model fitted to training set\")\n",
    "    \n",
    "    \n",
    "    # Uncomment to use the desired set of features\n",
    "    # X_test = X_test_vect_avg\n",
    "    # X_test = np.column_stack((X_test_vect_avg, test_profanity, test_politeness))\n",
    "    X_test = np.column_stack((X_test_vect_avg, test_profanity, test_politeness, test_genres_array))\n",
    "    \n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    print(\"INFO: predictiond on test set done\")\n",
    "    \n",
    "    \n",
    "#########################################\n",
    "    \n",
    "#     # Calculate class-wise accuracy\n",
    "#     class_wise_accuracy = {}\n",
    "#     for class_label in set(y_test):\n",
    "#         class_mask = y_test == class_label\n",
    "#         class_accuracy = accuracy_score(y_test[class_mask], y_pred[class_mask])\n",
    "#         class_wise_accuracy[class_label] = class_accuracy\n",
    "#     print(\"Class-wise Accuracy:\")\n",
    "#     print(class_wise_accuracy)\n",
    "    \n",
    "#########################################\n",
    "\n",
    "    \n",
    "    # Finding top-1, top-3 and top-5 Accuracy\n",
    "    ##################\n",
    "    # Step 1: Get probability scores for each class label\n",
    "    y_probabilities = rf_model.predict_proba(X_test)\n",
    "    print(\"INFO: y_probabilities created\")\n",
    "\n",
    "    # Step 2: Sort probability scores in descending order to get top 3 probabilities and labels\n",
    "    top3_indices = np.argsort(y_probabilities, axis=1)[:, -3:]\n",
    "    top3_labels = rf_model.classes_[top3_indices]\n",
    "    \n",
    "    top5_indices = np.argsort(y_probabilities, axis=1)[:, -5:]\n",
    "    top5_labels = rf_model.classes_[top5_indices]\n",
    "    print(\"INFO: top 3 and 5 labels created\")\n",
    "    \n",
    "    # Convert y_test to a NumPy array\n",
    "    y_test_np = y_test.values.reshape(-1, 1)\n",
    "\n",
    "    # Step 3: Check if true label is in the top 3 probabilities for each sample\n",
    "    correct_top3_predictions = np.any(top3_labels == y_test_np, axis=1)\n",
    "    correct_top5_predictions = np.any(top5_labels == y_test_np, axis=1)\n",
    "\n",
    "    # Step 4: Calculate accuracy based on top 3 probabilities\n",
    "    top3_accuracy = np.mean(correct_top3_predictions)\n",
    "    top5_accuracy = np.mean(correct_top5_predictions)\n",
    "    ##################\n",
    "    print(\"INFO: top 3 and 5 accuracy calculated\")\n",
    "    \n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Top 3 Accuracy: {top3_accuracy:.4f}\")\n",
    "    print(f\"Top 5 Accuracy: {top5_accuracy:.4f}\")\n",
    "    \n",
    "    return rf_model, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c208820d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Classify_SVM(X_train_vect_avg, train_genres_array, train_profanity, train_politeness, X_test_vect_avg, test_genres_array, test_profanity, test_politeness, y_train, y_test): \n",
    "########################################\n",
    "#     #Hyperparameter Tuning           #\n",
    "########################################\n",
    "#     param_grid = {\n",
    "#         'C': [0.1, 1, 10],\n",
    "#         'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "#         'gamma': [0.1, 1, 'scale'],\n",
    "#         'probability': [True]\n",
    "#     }\n",
    "\n",
    "#     # Perform Grid Search with cross-validation\n",
    "#     grid_search = GridSearchCV(svm_model, param_grid, cv=5)\n",
    "#     grid_search.fit(X_train_vect_avg, y_train.values.ravel())\n",
    "\n",
    "#     # Get the best hyperparameters\n",
    "#     best_params = grid_search.best_params_\n",
    "#     print(\"INFO: Best Params\")\n",
    "#     print(best_params)\n",
    "\n",
    "#     # Initialize the SVM model with the best hyperparameters\n",
    "#     svm_model = SVC(**best_params)\n",
    "    \n",
    "#     Define the SVM model\n",
    "#     Using parameters with best perfromace after grid search\n",
    "########################################\n",
    "#     #Hyperparameter Tuning           #\n",
    "########################################\n",
    "\n",
    "    svm_model = SVC(kernel='poly', C=1.0, gamma='scale', probability=True , random_state = random.seed(1))\n",
    "    print(\"INFO: svm_model created\")\n",
    "    \n",
    "    # Uncomment to use the desired set of features\n",
    "    # X_train = X_train_vect_avg\n",
    "    # X_train = np.column_stack((X_train_vect_avg, train_profanity, train_politeness))\n",
    "    X_train = np.column_stack((X_train_vect_avg, train_profanity, train_politeness, train_genres_array))\n",
    "     \n",
    "    svm_model.fit(X_train, y_train.values.ravel())\n",
    "    print(\"INFO: svm_model fitted to training set\")\n",
    "    \n",
    "    # Uncomment to use the desired set of features\n",
    "    # X_test = X_test_vect_avg\n",
    "    # X_test = np.column_stack((X_test_vect_avg, test_profanity, test_politeness))\n",
    "    X_test = np.column_stack((X_test_vect_avg, test_profanity, test_politeness, test_genres_array))\n",
    "    \n",
    "    y_pred = svm_model.predict(X_test)\n",
    "    print(\"INFO: predictiond on test set done\")\n",
    "    \n",
    "    \n",
    "#########################################\n",
    "    # Calculate class-wise accuracy\n",
    "#     class_wise_accuracy = {}\n",
    "#     for class_label in set(y_test):\n",
    "#         class_mask = y_test == class_label\n",
    "#         class_accuracy = accuracy_score(y_test[class_mask], y_pred[class_mask])\n",
    "#         class_wise_accuracy[class_label] = class_accuracy\n",
    "#     print(\"Class-wise Accuracy:\")\n",
    "#     print(class_wise_accuracy)\n",
    "    \n",
    "#########################################\n",
    "    \n",
    "    \n",
    "    # Finding top-1, top-3 and top-5 Accuracy\n",
    "    # Step 1: Get probability scores for each class label\n",
    "    y_probabilities = svm_model.predict_proba(X_test)\n",
    "    print(\"INFO: y_probabilities created\")\n",
    "\n",
    "    # Step 2: Sort probability scores in descending order to get top 3 probabilities and labels\n",
    "    top3_indices = np.argsort(y_probabilities, axis=1)[:, -3:]\n",
    "    top3_labels = svm_model.classes_[top3_indices]\n",
    "    \n",
    "    top5_indices = np.argsort(y_probabilities, axis=1)[:, -5:]\n",
    "    top5_labels = svm_model.classes_[top5_indices]\n",
    "    print(\"INFO: top 3 and 5 labels created\")\n",
    "    \n",
    "    # Convert y_test to a NumPy array\n",
    "    y_test_np = y_test.values.reshape(-1, 1)\n",
    "\n",
    "    # Step 3: Check if true label is in the top 3 probabilities for each sample\n",
    "    correct_top3_predictions = np.any(top3_labels == y_test_np, axis=1)\n",
    "    correct_top5_predictions = np.any(top5_labels == y_test_np, axis=1)\n",
    "\n",
    "    # Step 4: Calculate accuracy based on top 3 probabilities\n",
    "    top3_accuracy = np.mean(correct_top3_predictions)\n",
    "    top5_accuracy = np.mean(correct_top5_predictions)\n",
    "    print(\"INFO: top 3 and 5 accuracy calculated\")\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Top 3 Accuracy: {top3_accuracy:.4f}\")\n",
    "    print(f\"Top 5 Accuracy: {top5_accuracy:.4f}\")\n",
    "    \n",
    "    return svm_model, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c40da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Classify_LinearSVM(X_train_vect_avg, train_genres_array, train_profanity, train_politeness, X_test_vect_avg, test_genres_array, test_profanity, test_politeness, y_train, y_test): \n",
    "    clf = LinearSVC(max_iter=1000, random_state = random.seed(1)) \n",
    "    svm_model = CalibratedClassifierCV(base_estimator = clf, method='sigmoid') #, cv='prefit')\n",
    "    \n",
    "    print(\"INFO: LinearSVC_model created\")\n",
    "    \n",
    "    # Uncomment to use the desired set of features\n",
    "    # X_train = X_train_vect_avg\n",
    "    # X_train = np.column_stack((X_train_vect_avg, train_profanity, train_politeness))\n",
    "    X_train = np.column_stack((X_train_vect_avg, train_profanity, train_politeness, train_genres_array))\n",
    "     \n",
    "    svm_model.fit(X_train, y_train.values.ravel())\n",
    "    print(\"INFO: LinearSVC_model fitted to training set\")\n",
    "    \n",
    "    # Uncomment to use the desired set of features\n",
    "    # X_test = X_test_vect_avg\n",
    "    # X_test = np.column_stack((X_test_vect_avg, test_profanity, test_politeness))\n",
    "    X_test = np.column_stack((X_test_vect_avg, test_profanity, test_politeness, test_genres_array))\n",
    "    \n",
    "    y_pred = svm_model.predict(X_test)\n",
    "    print(\"INFO: predictiond on test set done\")\n",
    "    \n",
    "    \n",
    "    #########################################\n",
    "    \n",
    "    # Calculate class-wise accuracy\n",
    "    class_wise_accuracy = {}\n",
    "    for class_label in set(y_test):\n",
    "        class_mask = y_test == class_label\n",
    "        class_accuracy = accuracy_score(y_test[class_mask], y_pred[class_mask])\n",
    "        class_wise_accuracy[class_label] = class_accuracy\n",
    "\n",
    "    print(\"Class-wise Accuracy:\")\n",
    "    print(class_wise_accuracy)\n",
    "    \n",
    "    #########################################\n",
    "    \n",
    "    \n",
    "    # Finding top-1, top-3 and top-5 Accuracy\n",
    "    # Step 1: Get probability scores for each class label\n",
    "    y_probabilities = svm_model.predict_proba(X_test)\n",
    "    print(\"INFO: y_probabilities created\")\n",
    "\n",
    "    # Step 2: Sort probability scores in descending order to get top 3 probabilities and labels\n",
    "    top3_indices = np.argsort(y_probabilities, axis=1)[:, -3:]\n",
    "    top3_labels = svm_model.classes_[top3_indices]\n",
    "    \n",
    "    top5_indices = np.argsort(y_probabilities, axis=1)[:, -5:]\n",
    "    top5_labels = svm_model.classes_[top5_indices]\n",
    "    print(\"INFO: top 3 and 5 labels created\")\n",
    "    \n",
    "    # Convert y_test to a NumPy array\n",
    "    y_test_np = y_test.values.reshape(-1, 1)\n",
    "\n",
    "    # Step 3: Check if true label is in the top 3 probabilities for each sample\n",
    "    correct_top3_predictions = np.any(top3_labels == y_test_np, axis=1)\n",
    "    correct_top5_predictions = np.any(top5_labels == y_test_np, axis=1)\n",
    "\n",
    "    # Step 4: Calculate accuracy based on top 3 probabilities\n",
    "    top3_accuracy = np.mean(correct_top3_predictions)\n",
    "    top5_accuracy = np.mean(correct_top5_predictions)\n",
    "    print(\"INFO: top 3 and 5 accuracy calculated\")\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Top 3 Accuracy: {top3_accuracy:.4f}\")\n",
    "    print(f\"Top 5 Accuracy: {top5_accuracy:.4f}\")\n",
    "    \n",
    "    return svm_model, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99356ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vect_avg(sentence_model, words, batch_df_col):\n",
    "    vect = np.array([np.array([sentence_model.wv[i] \n",
    "                                       for i in vectors if i in words])\n",
    "                             for vectors in batch_df_col])\n",
    "    \n",
    "    return vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e46e4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(raw_df):\n",
    "    res_df = invoke_genre_pre_processing(raw_df)\n",
    "    res_df = invoke_remove_line_with_substring(res_df, 'Lyrics')\n",
    "    res_df = remove_non_eng_and_songs_without_lyrics(res_df, 'Lyrics_without_junk')\n",
    "    res_df = invoke_profanity_politeness_score(res_df, 'Lyrics_without_junk')\n",
    "    res_df = remove_round_brackets(res_df, 'Lyrics_without_junk')\n",
    "    res_df = remove_square_brackets(res_df, 'Lyrics_without_rb')\n",
    "    res_df = fetch_nlc_removed_lyrics(res_df, 'Lyrics_without_sb')\n",
    "    res_df = to_lower_and_remove_punctuations(res_df, 'Lyrics_without_nlc')\n",
    "    res_df.to_pickle(\"./Datasets/finalproject/artist_genre_lyrics_preprocessed_df.pkl\")\n",
    "    \n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479676bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def orchestrate(df):\n",
    "    random.seed(1)\n",
    "    print(\"INFO: started main\")\n",
    "    \n",
    "    # Uncomment to perform pre-processing\n",
    "    # res_df = preprocess(artist_genre_lyrics_df) # data_df # artist_genre_lyrics_df # artist_data_midsize_df\n",
    "    # print(\"INFO: pre processing done\")\n",
    "    res_df = df\n",
    "    \n",
    "    # Uncomment to plot Artist vs Songs distribution\n",
    "    # n_songs_artists(res_df)\n",
    "    \n",
    "    vector_size = 100 #[100, 200, 300]\n",
    "    window_size = 25 #[5, 10, 15, 20, 25]\n",
    "    \n",
    "    #########################\n",
    "    # Used to create stratified smaller datasets\n",
    "    data_50, _ = train_test_split(res_df,\n",
    "                                  test_size=0.50,\n",
    "                                  stratify=res_df['Artist_Name'],\n",
    "                                  random_state=42)\n",
    "    #########################\n",
    "    \n",
    "    train, test = train_test_split(res_df, # data_50 use this value if smaller portion of dataset needed\n",
    "                                   test_size=0.2,\n",
    "                                   stratify=res_df['Artist_Name'], # data_50['Artist_Name'] use this value if smaller portion of dataset needed\n",
    "                                   random_state=42)\n",
    "    print(\"INFO: train-test split created\")\n",
    "\n",
    "    lyrics_model = Word2Vec_vectorize_complete_song(train['Lyrics_clean'], \n",
    "                                                      test['Lyrics_clean'], \n",
    "                                                      train['Artist_Name'], \n",
    "                                                      test['Artist_Name'],\n",
    "                                                      vector_size,\n",
    "                                                      window_size)\n",
    "    print(\"INFO: Word2Vec model created\")\n",
    "    \n",
    "    # Uncomment to plot t-SNE projection of Word2Vec vectors\n",
    "    # plot_with_matplotlib(lyrics_model, res_df)\n",
    "\n",
    "    w2v_weights = lyrics_model.wv.vectors\n",
    "    vocab_size, embedding_size = w2v_weights.shape\n",
    "\n",
    "    print(\"Vocabulary Size: {} - Embedding Dim: {} - Window Size: {}\".format(vocab_size, embedding_size, window_size))\n",
    "\n",
    "    words = set(lyrics_model.wv.index_to_key)\n",
    "\n",
    "    ##############\n",
    "    # Compute lyrics vectors by averaging the word vectors for the words contained in the lyrics\n",
    "    ##############\n",
    "    X_train_vect_avg = []\n",
    "    \n",
    "    # Define the batch size\n",
    "    batch_size = 1000\n",
    "\n",
    "    # Iterate through the dataframe in batches\n",
    "    num_records = len(train['Lyrics_clean'])\n",
    "    num_batches = (num_records // batch_size) + 1\n",
    "    \n",
    "    print(\"INFO: Working on train set\")\n",
    "    \n",
    "    for batch_num in range(num_batches):\n",
    "        print(\"Working on batch: \", batch_num)\n",
    "        start_idx = batch_num * batch_size\n",
    "        end_idx = min((batch_num + 1) * batch_size, num_records)\n",
    "\n",
    "        batch_df_col = train['Lyrics_clean'].iloc[start_idx:end_idx]\n",
    "        processed_data = generate_vect_avg(lyrics_model, words, batch_df_col)\n",
    "        \n",
    "        for v in processed_data:\n",
    "            if v.size:\n",
    "                X_train_vect_avg.append(v.mean(axis=0))\n",
    "            else:\n",
    "                X_train_vect_avg.append(np.zeros(vec_s, dtype=float))\n",
    "    \n",
    "    X_test_vect_avg = []\n",
    "    \n",
    "    # Iterate through the dataframe in batches\n",
    "    num_records = len(test['Lyrics_clean'])\n",
    "    num_batches = (num_records // batch_size) + 1\n",
    "    \n",
    "    print(\"INFO: Working on test set\")\n",
    "    \n",
    "    for batch_num in range(num_batches):\n",
    "        print(\"Working on batch: \", batch_num)\n",
    "        start_idx = batch_num * batch_size\n",
    "        end_idx = min((batch_num + 1) * batch_size, num_records)\n",
    "\n",
    "        batch_df_col = test['Lyrics_clean'].iloc[start_idx:end_idx]\n",
    "        processed_data = generate_vect_avg(lyrics_model, words, batch_df_col)\n",
    "        \n",
    "        for v in processed_data:\n",
    "            if v.size:\n",
    "                X_test_vect_avg.append(v.mean(axis=0))\n",
    "            else:\n",
    "                X_test_vect_avg.append(np.zeros(vec_s, dtype=float))\n",
    "    \n",
    "    # Uncomment the claasifier which you want to use. Currently using SVM\n",
    "#     print(\"####################################\")\n",
    "#     print(\"INFO: Calling RF classifier function\")\n",
    "#     print(\"####################################\")\n",
    "#     model, accuracy = Classify_RF(X_train_vect_avg, \n",
    "#                                   train['Genres_array'].tolist(), \n",
    "#                                   train['profanity'],\n",
    "#                                   train['politeness'],\n",
    "#                                   X_test_vect_avg, \n",
    "#                                   test['Genres_array'].tolist(), \n",
    "#                                   test['profanity'],\n",
    "#                                   test['politeness'],\n",
    "#                                   train['Artist_Name'], \n",
    "#                                   test['Artist_Name'])\n",
    "    \n",
    "#     print(\"####################################\")\n",
    "#     print(\"INFO: Calling SVM classifier function\")\n",
    "#     print(\"####################################\")\n",
    "#     model, accuracy = Classify_SVM(X_train_vect_avg, \n",
    "#                                    train['Genres_array'].tolist(), \n",
    "#                                    train['profanity'],\n",
    "#                                    train['politeness'],\n",
    "#                                    X_test_vect_avg,\n",
    "#                                    test['Genres_array'].tolist(), \n",
    "#                                    test['profanity'],\n",
    "#                                    test['politeness'],\n",
    "#                                    train['Artist_Name'],\n",
    "#                                    test['Artist_Name'])\n",
    "    \n",
    "    print(\"####################################\")\n",
    "    print(\"INFO: Calling Linear SVM classifier function\")\n",
    "    print(\"####################################\")\n",
    "    model, accuracy = Classify_LinearSVM(X_train_vect_avg, \n",
    "                                   train['Genres_array'].tolist(), \n",
    "                                   train['profanity'],\n",
    "                                   train['politeness'],\n",
    "                                   X_test_vect_avg,\n",
    "                                   test['Genres_array'].tolist(), \n",
    "                                   test['profanity'],\n",
    "                                   test['politeness'],\n",
    "                                   train['Artist_Name'],\n",
    "                                   test['Artist_Name'])\n",
    "    \n",
    "    print(\"INFO: control back to main\")\n",
    "    return model, lyrics_model, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e93f03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating vector for prompts\n",
    "def predict_artist(sentence_words, words, sentence_model, vec_size = 300):\n",
    "    tokenized_test_song_lyrics = sentence_words.split()    \n",
    "    X_pred_vect = [np.array([sentence_model.wv[i] for i in tokenized_test_song_lyrics if i in words])]\n",
    "                              \n",
    "    X_pred_vect_avg = []\n",
    "    for v in X_pred_vect:\n",
    "        if v.size:\n",
    "            X_pred_vect_avg.append(v.mean(axis=0))\n",
    "        else:\n",
    "            X_pred_vect_avg.append(np.zeros(vec_size, dtype=float))\n",
    "        \n",
    "    return X_pred_vect_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3022a0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test out prompts\n",
    "def test_prompts():\n",
    "    # Elton john: \"i not the man they think i am at home\"\n",
    "    # Billie Eilish: \"white shirt now red my bloody nose\"\n",
    "    sample_string = \"white shirt now red my bloody nose sleepin you on your tippy toes creepin around like no one knows think you so criminal bruises on both my knees for you don't say thank you or please i do what i want when i wanting to my soul so cynical\"\n",
    "    rf_model, sentence_model, words = orchestrate()\n",
    "    X_pred_vect_avg = predict_artist(sample_string, words, sentence_model)\n",
    "    \n",
    "    output = rf_model.predict(X_pred_vect_avg)\n",
    "    output_proba = rf_model.predict_proba(X_pred_vect_avg)\n",
    "    print(rf_model.classes_)\n",
    "    print(output_proba)\n",
    "    print(output)\n",
    "    \n",
    "test_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3940926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run code\n",
    "if __name__ == \"__main__\":\n",
    "    artist_genre_lyrics_df = pd.read_pickle(\"./artist_genre_lyrics_preprocessed_filtered_df.pkl\")\n",
    "    start = time.time()\n",
    "    rf_model, lyrics_model, words = main(artist_genre_lyrics_df)\n",
    "    tot_time = time.time() - start\n",
    "    print(tot_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
